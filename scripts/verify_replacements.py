import json
import os
import re


def load_replacement_map():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Ä—Ç—É –∑–∞–º–µ–Ω"""
    with open('knowledge_base/terms_map_comprehensive.json', 'r', encoding='utf-8') as f:
        return json.load(f)


def find_leaked_terms(text, replacement_map):
    """–ù–∞—Ö–æ–¥–∏—Ç —É—Ç–µ—á–∫–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    leaked_terms = []

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    original_terms = set(replacement_map.keys())

    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ (–±–µ–∑ –æ–∫–æ–Ω—á–∞–Ω–∏–π)
    base_terms = set()
    for term in original_terms:
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        base = re.sub(r'(s|es|ing|ed|er)$', '', term.lower())
        if len(base) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
            base_terms.add(base)

    for original in original_terms:
        # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (—Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞)
        if re.search(r'\b' + re.escape(original) + r'\b', text, re.IGNORECASE):
            leaked_terms.append(original)

    return leaked_terms


def verify_improved_replacements():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–º–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""

    replacement_map = load_replacement_map()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    with open('knowledge_base/processing_metadata_v2.json', 'r', encoding='utf-8') as f:
        processed_files = json.load(f)

    print("=== IMPROVED REPLACEMENT VERIFICATION ===")
    print(f"Total replacement terms: {len(replacement_map)}")
    print(f"Processed files: {len(processed_files)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞–º–µ–Ω
    print("\n=== SAMPLE REPLACEMENTS ===")
    import random
    sample_terms = random.sample(list(replacement_map.items()), 15)
    for original, replacement in sample_terms:
        print(f"  {original} ‚Üí {replacement}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ñ–∞–π–ª
    if processed_files:
        sample_file = random.choice(processed_files)
        print(f"\n=== SAMPLE FILE: {sample_file['processed_file']} ===")

        with open(sample_file['processed_file'], 'r', encoding='utf-8') as f:
            content = f.read()

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        preview = content[:500] + "..." if len(content) > 500 else content
        print(preview)

    # –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Ç–µ—á–µ–∫
    print("\n=== DETAILED LEAK CHECK ===")
    total_leaks = 0
    leak_details = {}

    for processed_file in processed_files:
        with open(processed_file['processed_file'], 'r', encoding='utf-8') as f:
            content = f.read()

        leaked_terms = find_leaked_terms(content, replacement_map)
        if leaked_terms:
            leak_details[processed_file['processed_file']] = leaked_terms
            total_leaks += len(leaked_terms)

    if leak_details:
        print(f"‚ö†Ô∏è  Found {total_leaks} leaked terms across {len(leak_details)} files:")
        for filepath, terms in list(leak_details.items())[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ñ–∞–π–ª–æ–≤
            print(f"  {os.path.basename(filepath)}: {', '.join(terms[:5])}{'...' if len(terms) > 5 else ''}")

        if len(leak_details) > 10:
            print(f"  ... and {len(leak_details) - 10} more files with leaks")
    else:
        print("‚úì No leaked terms found - excellent!")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n=== STATISTICS ===")
    print(f"Total files checked: {len(processed_files)}")
    print(f"Files with leaks: {len(leak_details)}")
    print(f"Total leaked terms: {total_leaks}")

    return len(leak_details) == 0


if __name__ == "__main__":
    success = verify_improved_replacements()
    if success:
        print("\nüéâ SUCCESS! All terms properly replaced.")
    else:
        print("\n‚ùå Some terms still need fixing.")